Exception in thread Thread-1 (_create_weight_func):
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 196, in _create_weight_func
    model_comm.create_shared_weights(device_id, rank)
RuntimeError: [TM][ERROR] CUDA runtime error: out of memory /lmdeploy/src/turbomind/utils/memory_utils.cu:32 

Exception in thread Thread-2 (_get_params):
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 226, in _get_params
    out = model_comm.get_params(device_id, rank)
RuntimeError: [TM][ERROR]  Assertion fail: /lmdeploy/src/turbomind/triton_backend/llama/LlamaTritonModel.cc:384 

OOM: UUID: GPU-e2404814-42eb-2128-e6cc-339375a383cdMB; Current used: 24964MB; Assigned Limit: 24576MB
0000:89:00.0 24566 24566
OOM: UUID: GPU-e2404814-42eb-2128-e6cc-339375a383cdMB; Current used: 24964MB; Assigned Limit: 24576MB
0000:89:00.0 24566 24566
Convert to turbomind format:   0%|          | 0/32 [00:00<?, ?it/s]Convert to turbomind format:   3%|▎         | 1/32 [00:00<00:30,  1.00it/s]Convert to turbomind format:   6%|▋         | 2/32 [00:01<00:16,  1.80it/s]Convert to turbomind format:   9%|▉         | 3/32 [00:02<00:23,  1.22it/s]Convert to turbomind format:  12%|█▎        | 4/32 [00:02<00:16,  1.67it/s]Convert to turbomind format:  16%|█▌        | 5/32 [00:02<00:12,  2.18it/s]Convert to turbomind format:  19%|█▉        | 6/32 [00:03<00:09,  2.69it/s]Convert to turbomind format:  22%|██▏       | 7/32 [00:03<00:08,  2.93it/s]Convert to turbomind format:  25%|██▌       | 8/32 [00:04<00:13,  1.77it/s]Convert to turbomind format:  28%|██▊       | 9/32 [00:04<00:10,  2.24it/s]Convert to turbomind format:  31%|███▏      | 10/32 [00:04<00:08,  2.73it/s]Convert to turbomind format:  34%|███▍      | 11/32 [00:04<00:06,  3.25it/s]Convert to turbomind format:  38%|███▊      | 12/32 [00:05<00:10,  1.85it/s]Convert to turbomind format:  41%|████      | 13/32 [00:06<00:08,  2.26it/s]Convert to turbomind format:  44%|████▍     | 14/32 [00:06<00:06,  2.71it/s]Convert to turbomind format:  47%|████▋     | 15/32 [00:06<00:05,  3.17it/s]Convert to turbomind format:  50%|█████     | 16/32 [00:06<00:04,  3.53it/s]Convert to turbomind format:  53%|█████▎    | 17/32 [00:07<00:07,  1.96it/s]Convert to turbomind format:  56%|█████▋    | 18/32 [00:08<00:05,  2.45it/s]Convert to turbomind format:  59%|█████▉    | 19/32 [00:08<00:04,  2.96it/s]Convert to turbomind format:  62%|██████▎   | 20/32 [00:08<00:03,  3.47it/s]Convert to turbomind format:  66%|██████▌   | 21/32 [00:09<00:05,  1.92it/s]Convert to turbomind format:  69%|██████▉   | 22/32 [00:09<00:04,  2.44it/s]Convert to turbomind format:  72%|███████▏  | 23/32 [00:09<00:03,  2.98it/s]Convert to turbomind format:  75%|███████▌  | 24/32 [00:09<00:02,  3.54it/s]Convert to turbomind format:  78%|███████▊  | 25/32 [00:10<00:01,  3.90it/s]Convert to turbomind format:  81%|████████▏ | 26/32 [00:11<00:02,  2.16it/s]Convert to turbomind format:  84%|████████▍ | 27/32 [00:11<00:01,  2.70it/s]Convert to turbomind format:  88%|████████▊ | 28/32 [00:11<00:01,  3.31it/s]Convert to turbomind format:  91%|█████████ | 29/32 [00:11<00:00,  3.86it/s]Convert to turbomind format:  94%|█████████▍| 30/32 [00:12<00:01,  1.96it/s]Convert to turbomind format:  97%|█████████▋| 31/32 [00:12<00:00,  2.50it/s]Convert to turbomind format: 100%|██████████| 32/32 [00:12<00:00,  3.03it/s]                                                                            [WARNING] gemm_config.in is not found; using default GEMM algo
HINT:    Please open [93m[1mhttp://127.0.0.1:23333[0m in a browser for detailed api usage!!!
HINT:    Please open [93m[1mhttp://127.0.0.1:23333[0m in a browser for detailed api usage!!!
HINT:    Please open [93m[1mhttp://127.0.0.1:23333[0m in a browser for detailed api usage!!!
INFO:     Started server process [416841]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:23333 (Press CTRL+C to quit)
Exception in thread Thread-1 (_create_weight_func):
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 196, in _create_weight_func
    model_comm.create_shared_weights(device_id, rank)
RuntimeError: [TM][ERROR] CUDA runtime error: out of memory /lmdeploy/src/turbomind/utils/memory_utils.cu:32 

Exception in thread Thread-2 (_get_params):
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 226, in _get_params
    out = model_comm.get_params(device_id, rank)
RuntimeError: [TM][ERROR]  Assertion fail: /lmdeploy/src/turbomind/triton_backend/llama/LlamaTritonModel.cc:384 

INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [416841]
Convert to turbomind format:   0%|          | 0/32 [00:00<?, ?it/s]Convert to turbomind format:   3%|▎         | 1/32 [00:00<00:24,  1.29it/s]Convert to turbomind format:   6%|▋         | 2/32 [00:01<00:16,  1.83it/s]Convert to turbomind format:   9%|▉         | 3/32 [00:02<00:23,  1.21it/s]Convert to turbomind format:  12%|█▎        | 4/32 [00:02<00:16,  1.72it/s]Convert to turbomind format:  16%|█▌        | 5/32 [00:02<00:11,  2.26it/s]Convert to turbomind format:  19%|█▉        | 6/32 [00:02<00:09,  2.81it/s]Convert to turbomind format:  22%|██▏       | 7/32 [00:03<00:07,  3.23it/s]Convert to turbomind format:  25%|██▌       | 8/32 [00:03<00:10,  2.19it/s]Convert to turbomind format:  28%|██▊       | 9/32 [00:04<00:08,  2.84it/s]Convert to turbomind format:  31%|███▏      | 10/32 [00:04<00:06,  3.54it/s]Convert to turbomind format:  34%|███▍      | 11/32 [00:04<00:05,  3.83it/s]Convert to turbomind format:  38%|███▊      | 12/32 [00:05<00:08,  2.24it/s]Convert to turbomind format:  41%|████      | 13/32 [00:05<00:06,  2.90it/s]Convert to turbomind format:  44%|████▍     | 14/32 [00:05<00:05,  3.57it/s]Convert to turbomind format:  47%|████▋     | 15/32 [00:05<00:03,  4.28it/s]Convert to turbomind format:  50%|█████     | 16/32 [00:05<00:03,  4.56it/s]Convert to turbomind format:  53%|█████▎    | 17/32 [00:06<00:06,  2.39it/s]Convert to turbomind format:  56%|█████▋    | 18/32 [00:06<00:05,  2.67it/s]Convert to turbomind format:  59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s]Convert to turbomind format:  62%|██████▎   | 20/32 [00:07<00:03,  3.35it/s]Convert to turbomind format:  66%|██████▌   | 21/32 [00:08<00:05,  2.07it/s]Convert to turbomind format:  69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s]Convert to turbomind format:  72%|███████▏  | 23/32 [00:08<00:02,  3.03it/s]Convert to turbomind format:  75%|███████▌  | 24/32 [00:08<00:02,  3.64it/s]Convert to turbomind format:  78%|███████▊  | 25/32 [00:08<00:01,  4.45it/s]Convert to turbomind format:  81%|████████▏ | 26/32 [00:09<00:02,  2.67it/s]Convert to turbomind format:  84%|████████▍ | 27/32 [00:09<00:01,  3.05it/s]Convert to turbomind format:  88%|████████▊ | 28/32 [00:10<00:01,  3.56it/s]Convert to turbomind format:  91%|█████████ | 29/32 [00:10<00:00,  4.32it/s]Convert to turbomind format:  94%|█████████▍| 30/32 [00:11<00:00,  2.32it/s]Convert to turbomind format:  97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]Convert to turbomind format: 100%|██████████| 32/32 [00:11<00:00,  3.40it/s]                                                                            [WARNING] gemm_config.in is not found; using default GEMM algo
HINT:    Please open [93m[1mhttp://127.0.0.1:23333[0m in a browser for detailed api usage!!!
HINT:    Please open [93m[1mhttp://127.0.0.1:23333[0m in a browser for detailed api usage!!!
HINT:    Please open [93m[1mhttp://127.0.0.1:23333[0m in a browser for detailed api usage!!!
INFO:     Started server process [417768]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:23333 (Press CTRL+C to quit)
Exception in thread Thread-1 (_create_weight_func):
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 196, in _create_weight_func
    model_comm.create_shared_weights(device_id, rank)
RuntimeError: [TM][ERROR] CUDA runtime error: out of memory /lmdeploy/src/turbomind/utils/memory_utils.cu:32 

Exception in thread Thread-2 (_get_params):
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 226, in _get_params
    out = model_comm.get_params(device_id, rank)
RuntimeError: [TM][ERROR]  Assertion fail: /lmdeploy/src/turbomind/triton_backend/llama/LlamaTritonModel.cc:384 

INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [417768]
Convert to turbomind format:   0%|          | 0/32 [00:00<?, ?it/s]Convert to turbomind format:   3%|▎         | 1/32 [00:00<00:23,  1.31it/s]Convert to turbomind format:   6%|▋         | 2/32 [00:00<00:13,  2.30it/s]Convert to turbomind format:   9%|▉         | 3/32 [00:01<00:18,  1.55it/s]Convert to turbomind format:  12%|█▎        | 4/32 [00:02<00:13,  2.08it/s]Convert to turbomind format:  16%|█▌        | 5/32 [00:02<00:09,  2.76it/s]Convert to turbomind format:  19%|█▉        | 6/32 [00:02<00:08,  2.94it/s]Convert to turbomind format:  22%|██▏       | 7/32 [00:02<00:07,  3.32it/s]Convert to turbomind format:  25%|██▌       | 8/32 [00:03<00:12,  1.91it/s]Convert to turbomind format:  28%|██▊       | 9/32 [00:03<00:09,  2.41it/s]Convert to turbomind format:  31%|███▏      | 10/32 [00:04<00:07,  2.85it/s]Convert to turbomind format:  34%|███▍      | 11/32 [00:04<00:06,  3.32it/s]Exception in thread Thread-1 (_create_weight_func):
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 196, in _create_weight_func
    model_comm.create_shared_weights(device_id, rank)
RuntimeError: [TM][ERROR] CUDA runtime error: out of memory /lmdeploy/src/turbomind/utils/memory_utils.cu:32 

Exception in thread Thread-2 (_get_params):
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 226, in _get_params
    out = model_comm.get_params(device_id, rank)
RuntimeError: [TM][ERROR]  Assertion fail: /lmdeploy/src/turbomind/triton_backend/llama/LlamaTritonModel.cc:384 

Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/bin/lmdeploy", line 8, in <module>
    sys.exit(run())
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/entrypoint.py", line 26, in run
    args.run(args)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/serve.py", line 284, in api_server
    run_api_server(args.model_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/serve/openai/api_server.py", line 1056, in serve
    VariableInterface.async_engine = pipeline_class(
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/serve/async_engine.py", line 188, in __init__
    self._build_turbomind(model_path=model_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/serve/async_engine.py", line 237, in _build_turbomind
    self.engine = tm.TurboMind.from_pretrained(
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 398, in from_pretrained
    return cls(model_path=pretrained_model_name_or_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 167, in __init__
    self.model_comm = self._from_hf(model_source=model_source,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 310, in _from_hf
    output_model.export()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/base.py", line 267, in export
    self.export_transformer_block(bin, i)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/fp.py", line 56, in export_transformer_block
    qw, kw, vw, ow = transpose_tensor([qw, kw, vw, ow])
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/fp.py", line 13, in transpose_tensor
    output = [x.cuda().t() for x in input]
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/fp.py", line 13, in <listcomp>
    output = [x.cuda().t() for x in input]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.99 GiB of which 28.00 MiB is free. Process 3106212 has 606.00 MiB memory in use. Process 3281777 has 1.11 GiB memory in use. Process 3301072 has 882.00 MiB memory in use. Process 3326853 has 14.89 GiB memory in use. Process 3331535 has 7.10 GiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
                                                                            Convert to turbomind format:   0%|          | 0/32 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/bin/lmdeploy", line 8, in <module>
    sys.exit(run())
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/entrypoint.py", line 26, in run
    args.run(args)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/serve.py", line 284, in api_server
    run_api_server(args.model_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/serve/openai/api_server.py", line 1056, in serve
    VariableInterface.async_engine = pipeline_class(
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/serve/async_engine.py", line 188, in __init__
    self._build_turbomind(model_path=model_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/serve/async_engine.py", line 237, in _build_turbomind
    self.engine = tm.TurboMind.from_pretrained(
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 398, in from_pretrained
    return cls(model_path=pretrained_model_name_or_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 167, in __init__
    self.model_comm = self._from_hf(model_source=model_source,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 310, in _from_hf
    output_model.export()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/base.py", line 265, in export
    self.export_misc(bin)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/base.py", line 296, in export_misc
    self.export_weight(emb, 'tok_embeddings.weight')
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/base.py", line 211, in export_weight
    torch_tensor = param.cuda().contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 724.00 MiB. GPU 0 has a total capacty of 23.99 GiB of which 102.00 MiB is free. Process 3106212 has 606.00 MiB memory in use. Process 3281777 has 1.11 GiB memory in use. Process 3301072 has 882.00 MiB memory in use. Process 3331535 has 7.10 GiB memory in use. Process 3340825 has 14.82 GiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
                                                                   Convert to turbomind format:   0%|          | 0/32 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/bin/lmdeploy", line 8, in <module>
    sys.exit(run())
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/entrypoint.py", line 26, in run
    args.run(args)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/serve.py", line 284, in api_server
    run_api_server(args.model_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/serve/openai/api_server.py", line 1056, in serve
    VariableInterface.async_engine = pipeline_class(
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/serve/async_engine.py", line 188, in __init__
    self._build_turbomind(model_path=model_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/serve/async_engine.py", line 237, in _build_turbomind
    self.engine = tm.TurboMind.from_pretrained(
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 398, in from_pretrained
    return cls(model_path=pretrained_model_name_or_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 167, in __init__
    self.model_comm = self._from_hf(model_source=model_source,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 310, in _from_hf
    output_model.export()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/base.py", line 265, in export
    self.export_misc(bin)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/base.py", line 296, in export_misc
    self.export_weight(emb, 'tok_embeddings.weight')
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/target_model/base.py", line 211, in export_weight
    torch_tensor = param.cuda().contiguous()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 724.00 MiB. GPU 0 has a total capacty of 23.99 GiB of which 102.00 MiB is free. Process 3106212 has 606.00 MiB memory in use. Process 3281777 has 1.11 GiB memory in use. Process 3301072 has 882.00 MiB memory in use. Process 3331535 has 7.10 GiB memory in use. Process 3342918 has 14.82 GiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
                                                                   